{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import re\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tag Tweets as HI,EN,CME,CMH or CMEQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweet_types = {}\n",
    "meta = {}\n",
    "with open('Datasheet.csv','r') as f:\n",
    "    for x in f:\n",
    "        tuples=x.split(',')\n",
    "        en_count=0\n",
    "        hi_count=0\n",
    "        total_word_count=0\n",
    "        other_count=0\n",
    "        ne_count=0\n",
    "        meta_data=[]\n",
    "        for i in range(1, len(tuples)) :\n",
    "            r = tuples[i].split(':')\n",
    "            s_index = int(r[0])\n",
    "            e_index = int(r[1])\n",
    "            w_type = str(r[2])\n",
    "            count = 1\n",
    "            meta_data.append((s_index,e_index,w_type))\n",
    "            if(w_type == 'EN'):\n",
    "                en_count+=count\n",
    "            elif w_type == 'HI' :\n",
    "                hi_count+= count\n",
    "            elif w_type == 'OTHER' :\n",
    "                other_count+=count\n",
    "            elif w_type == 'NE' :\n",
    "                ne_count+=count\n",
    "        meta[tuples[0]]=meta_data\n",
    "        total_word_count=en_count+hi_count\n",
    "            \n",
    "        en_ratio = float(en_count)/float(total_word_count)\n",
    "        hi_ratio = float(hi_count)/float(total_word_count)\n",
    "        t='None'\n",
    "        if(en_ratio>.9):\n",
    "            t='EN'\n",
    "        elif hi_ratio > .9:\n",
    "            t='HI'\n",
    "        elif hi_ratio > .5:\n",
    "            t='CMH'\n",
    "        elif en_ratio > .5:\n",
    "            t='CME'\n",
    "        elif en_ratio == .5:\n",
    "            t='CMEQ'\n",
    "        \n",
    "        tweet_types[tuples[0]]= t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 243347/243347 [00:01<00:00, 152513.59it/s]\n"
     ]
    }
   ],
   "source": [
    "filtered_tweets={}\n",
    "with open('data.csv','rU') as f:\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    for tweet_id,user_id,tweet in reader:\n",
    "        filtered_words = []\n",
    "        for s,e,t in meta[tweet_id]:\n",
    "            word = tweet[s-1:e]\n",
    "            if t=='HI' and word.lower().startswith('main'):\n",
    "                continue\n",
    "            filtered_words.append(word)\n",
    "        filtered_tweets[tweet_id]=' '.join(filtered_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize, filter stopwords and stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PUNCTUATION = set(string.punctuation)\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "STEMMER = PorterStemmer()\n",
    "\n",
    "# Function to break text into \"tokens\", lowercase them, remove punctuation and stopwords, and stem them\n",
    "def tokenize(text):\n",
    "    tokens = word_tokenize(re.sub('[^A-Za-z0-9]+', ' ', text))\n",
    "    lowercased = [t.lower() for t in tokens]\n",
    "    no_punctuation = []\n",
    "    for word in lowercased:\n",
    "        punct_removed = ''.join([letter for letter in word if not letter in PUNCTUATION])\n",
    "        no_punctuation.append(punct_removed)\n",
    "    no_stopwords = [w for w in no_punctuation if not w in STOPWORDS]\n",
    "    stemmed = [STEMMER.stem(w) for w in no_stopwords]\n",
    "    return [w for w in stemmed if w]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Words to be ranked \n",
    "\n",
    "Read the words to be ranked from input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 230/230 [00:00<00:00, 4711.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the words to be ranked from input.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print 'Reading the words to be ranked from input.txt'\n",
    "stemmed_key_words = []\n",
    "key_words = []\n",
    "with open('input.txt','rU') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for key_word, in reader:\n",
    "        key_words.append(key_word)\n",
    "        stemmed_key_words.append(tokenize(key_word)[0])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify required HashTags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 243347/243347 [00:58<00:00, 4151.23it/s]\n"
     ]
    }
   ],
   "source": [
    "req_hash_tags=set()\n",
    "with open('data.csv','rU') as f:\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    for tweet_id,user_id,tweet in reader:\n",
    "        words = tweet.split(\" \")\n",
    "        hash_tags = []\n",
    "        for word in words:\n",
    "            word = word.lower()\n",
    "            if word.startswith('#'):\n",
    "                hash_tags.append(word)\n",
    "        if len(hash_tags)>0:\n",
    "            words = tokenize(tweet)\n",
    "            for word in words:\n",
    "                if word in stemmed_key_words:\n",
    "                    for hash_tag in hash_tags:\n",
    "                        req_hash_tags.add(hash_tag)\n",
    "                    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Metric based on HI and CMH HashTags only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 243347/243347 [00:12<00:00, 18983.45it/s]\n"
     ]
    }
   ],
   "source": [
    "word_users = {}\n",
    "with open('data.csv','rU') as f:\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    for tweet_id,user_id,tweet in reader:\n",
    "        words = tweet.split(\" \")\n",
    "        hash_tag_present=False\n",
    "        for word in words:\n",
    "            if(word.startswith('#')):\n",
    "                if word in req_hash_tags:\n",
    "                    hash_tag_present=True\n",
    "                    break\n",
    "        \n",
    "        if (tweet_types[tweet_id]!='HI' and tweet_types[tweet_id]!= 'CMH') and hash_tag_present==False:\n",
    "            continue\n",
    "        \n",
    "        words = tokenize(filtered_tweets[tweet_id])\n",
    "        \n",
    "        for word in words:\n",
    "            word = word.lower()\n",
    "            if word in stemmed_key_words :\n",
    "                tweet_type = tweet_types[tweet_id]\n",
    "                if word not in word_users:\n",
    "                    word_users[word] = {}                    \n",
    "                if tweet_type not in word_users[word]:\n",
    "                    word_users[word][tweet_type] = set([user_id])\n",
    "                else:\n",
    "                    word_users[word][tweet_type].add(user_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_metric_dict = {}\n",
    "u_en_dict = {}\n",
    "u_hi_dict = {}\n",
    "u_cmh_dict = {}\n",
    "for word,type_user_counts in word_users.items():\n",
    "    u_en = 0.0\n",
    "    if 'EN' in type_user_counts:\n",
    "        u_en = float(len(type_user_counts['EN']))\n",
    "    u_hi = 0.0\n",
    "    if 'HI' in type_user_counts:\n",
    "        u_hi = float(len(type_user_counts['HI']))\n",
    "    u_cmh = 0.0\n",
    "    if 'CMH' in type_user_counts:\n",
    "        u_cmh = float(len(type_user_counts['CMH']))\n",
    "    user_metric = ((u_hi+u_cmh)/u_en)\n",
    "    u_en_dict[word] = u_en\n",
    "    u_hi_dict[word] = u_hi\n",
    "    u_cmh_dict[word] = u_cmh\n",
    "    user_metric_dict[word] = user_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet Metric based on Hindi and CMH Hash Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 243347/243347 [00:12<00:00, 19122.92it/s]\n"
     ]
    }
   ],
   "source": [
    "word_tweets = {}\n",
    "with open('data.csv','rU') as f:\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    for tweet_id,user_id,tweet in reader:\n",
    "        \n",
    "        words = tweet.split(\" \")\n",
    "        hash_tag_present=False\n",
    "        for word in words:\n",
    "            if(word.startswith('#')):\n",
    "                if word in req_hash_tags:\n",
    "                    hash_tag_present=True\n",
    "                    break\n",
    "        \n",
    "        if (tweet_types[tweet_id]!='HI' and tweet_types[tweet_id]!= 'CMH') and hash_tag_present==False:\n",
    "            continue\n",
    "        \n",
    "        words = tokenize(filtered_tweets[tweet_id])\n",
    "\n",
    "        for word in words:\n",
    "            word = word.lower()\n",
    "            if word in stemmed_key_words :\n",
    "                tweet_type = tweet_types[tweet_id]\n",
    "                if word not in word_tweets:\n",
    "                    word_tweets[word] = {}                    \n",
    "                if tweet_type not in word_tweets[word]:\n",
    "                    word_tweets[word][tweet_type] = set([tweet_id])\n",
    "                else:\n",
    "                    word_tweets[word][tweet_type].add(tweet_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweet_metric_dict = {}\n",
    "t_en_dict = {}\n",
    "t_hi_dict = {}\n",
    "t_cmh_dict = {}\n",
    "for word,type_tweet_counts in word_tweets.items():\n",
    "    t_en = 0.0\n",
    "    if 'EN' in type_tweet_counts:\n",
    "        t_en = float(len(type_tweet_counts['EN']))\n",
    "    t_hi = 0.0\n",
    "    if 'HI' in type_tweet_counts:\n",
    "        t_hi = float(len(type_tweet_counts['HI']))\n",
    "    t_cmh = 0.0\n",
    "    if 'CMH' in type_tweet_counts:\n",
    "        t_cmh = float(len(type_tweet_counts['CMH']))\n",
    "    tweet_metric = ((t_hi+t_cmh)/t_en)\n",
    "    t_en_dict[word] = t_en\n",
    "    t_hi_dict[word] = t_hi\n",
    "    t_cmh_dict[word] = t_cmh\n",
    "    tweet_metric_dict[word] = tweet_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final metric calculation\n",
    "Final metric is calculated as mean of tweet metric and user metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_ranks = []\n",
    "for key_word,stemmed_key_word in zip(key_words,stemmed_key_words):\n",
    "    final_metric = (user_metric_dict[stemmed_key_word]+tweet_metric_dict[stemmed_key_word])/2\n",
    "    final_ranks.append((key_word,final_metric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_rank_list = [s[0] for s in sorted(final_ranks,key=lambda l:l[1],reverse=True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write output to the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('output.csv','w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows([(e[1],e[0]+1) for e in enumerate(word_rank_list)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
